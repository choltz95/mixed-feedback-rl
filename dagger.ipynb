{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random, grad, vmap, jit, tree_multimap, tree_map\n",
    "from jax.experimental import stax, optimizers\n",
    "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Tanh, Flatten, LogSoftmax, Softmax\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from policy import Policy\n",
    "from learners import pwl, get_network\n",
    "from utils import make_plot, make_envs, plot_and_evaluate\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, axis\n",
    "\n",
    "import cloudpickle\n",
    "import h5py\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, math, time, pickle, itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## verify Jax is using the GPU\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate global rng for reproducability\n",
    "rng = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_expert_params = tree_map(jnp.array,pickle.load(open('cartpolev1_ppo2_expert.pkl','rb')))\n",
    "lander_expert_params = tree_map(jnp.array,pickle.load(open('lander_ppo2_expert.pkl','rb')))\n",
    "hopper_expert_params = tree_map(jnp.array,pickle.load(open('halfcheetah_ppo2_tanh_expert.pkl','rb')))\n",
    "half_cheetah_expert_params = tree_map(jnp.array,pickle.load(open('halfcheetah_ppo2_tanh_expert.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, envs = make_envs('CartPole-v1', 8)\n",
    "#env, envs = make_envs('LunarLander-v2')\n",
    "#env, envs = make_envs('HalfCheetah-v2')\n",
    "discrete = True\n",
    "\n",
    "set_state = False\n",
    "if hasattr(env, 'sim'):\n",
    "    invert_op = getattr(env.sim, \"set_state\", None)\n",
    "    if callable(invert_op):\n",
    "        set_state = True\n",
    "\n",
    "env_expert, env_noisy_expert, env_dagger, \\\n",
    "env_active_dagger, env_aggrevate, env_active_aggrevate, env_uncactive_aggrevate, env_uncactive_dagger = envs\n",
    "\n",
    "max_steps = env.spec.max_episode_steps\n",
    "\n",
    "init_obs = env.reset()\n",
    "in_shape = (-1,) + (init_obs.shape[0],)\n",
    "\n",
    "environments = [env_expert, env_noisy_expert, env_dagger, \n",
    "                env_active_dagger, env_aggrevate, env_active_aggrevate,\n",
    "                env_uncactive_aggrevate, env_uncactive_dagger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expname = './experts/HalfCheetah-v2.pkl'\n",
    "expert_network = 'cartpole'\n",
    "expert_params = cartpole_expert_params\n",
    "net_init_expert, net_apply_expert, net_walk_expert = get_network('{}-expert'.format(expert_network))\n",
    "#m = cloudpickle.load(open(expname,'rb'))['GaussianPolicy']['obsnorm']['Standardizer']['mean_1_D']\n",
    "#ms= cloudpickle.load(open(expname,'rb'))['GaussianPolicy']['obsnorm']['Standardizer']['meansq_1_D']\n",
    "m = 0.0\n",
    "ms = 1.0\n",
    "std = np.sqrt(np.maximum(0.0, ms - np.square(m)))\n",
    "\n",
    "net_init, net_apply, net_walk = get_network('agent-small',outshape=2)\n",
    "\n",
    "out_shape, init_params = net_init_expert(rng, in_shape)\n",
    "expert = Policy(rng, in_shape, (net_init_expert, net_apply_expert, net_walk_expert),name='expert', discrete=discrete, trainable=False)\n",
    "expert.params=expert_params\n",
    "expert.std_mean = m\n",
    "expert.std_std = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions_init = 1\n",
    "init_action, _, _ = expert.take_action(init_obs)\n",
    "obs_data = jnp.array([init_obs])\n",
    "act_data = jnp.array([init_action])\n",
    "act_data = act_data.reshape(n_actions_init, act_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%prun -s cumulative\n",
    "num_episodes = 10\n",
    "num_rollouts = 10\n",
    "b = 0.5 # b = 0.5 for , 0.1 for cheetah\n",
    "b2 = 1.0\n",
    "dsct_fctr = 0.95\n",
    "\n",
    "obs_data = jnp.array([init_obs])\n",
    "act_data = jnp.array([init_action])\n",
    "act_data = act_data.reshape(n_actions_init, act_data.shape[1])\n",
    "\n",
    "expert = Policy(rng, in_shape, (net_init_expert, net_apply_expert, net_walk_expert), \n",
    "                env=env_expert, name='expert', discrete=discrete, trainable=False)\n",
    "expert.params = expert_params\n",
    "#expert.std_mean = m\n",
    "#expert.std_std = std\n",
    "\n",
    "#dagger = Policy(rng, in_shape, (net_init, net_apply, net_walk),env=env_dagger, name='dagger', discrete=discrete)\n",
    "active_dagger = Policy(rng, in_shape, (net_init, net_apply, net_walk), \n",
    "                       env=env_active_dagger, name='activedagger', \n",
    "                       discrete=discrete, active=True, constrained=True)\n",
    "#aggrevate = Policy(rng, in_shape, (net_init, net_apply, net_walk),\n",
    "#                   env=env_aggrevate, name='aggrevate', \n",
    "#                   discrete=discrete, cost_sensitive=True)\n",
    "active_aggrevate = Policy(rng, in_shape, (net_init, net_apply, net_walk),\n",
    "                          env=env_active_aggrevate, name='activeaggrevate', \n",
    "                          discrete=discrete, active=True, constrained=True, cost_sensitive=True)\n",
    "\n",
    "dagger = Policy(rng, in_shape, (net_init, net_apply, net_walk), \n",
    "                       env=env_dagger, name='dagger', \n",
    "                       discrete=discrete, active=True, constrained=False) \n",
    "\n",
    "aggrevate = Policy(rng, in_shape, (net_init, net_apply, net_walk),\n",
    "                          env=env_aggrevate, name='aggrevate', \n",
    "                          discrete=discrete, active=True, constrained=False, cost_sensitive=True)\n",
    "\n",
    "unc_active_dagger = Policy(rng, in_shape, (net_init, net_apply, net_walk), \n",
    "                       env=env_uncactive_dagger, name='uncactivedagger', \n",
    "                       discrete=discrete, active=True, constrained=False)\n",
    "\n",
    "unc_active_aggrevate = Policy(rng, in_shape, (net_init, net_apply, net_walk),\n",
    "                          env=env_uncactive_aggrevate, name='uncactiveaggrevate', \n",
    "                          discrete=discrete, active=True, constrained=False, cost_sensitive=True)\n",
    "\n",
    "#innactive_policies = [expert, dagger, aggrevate]\n",
    "innactive_policies = [expert]\n",
    "#active_policies = [active_dagger, active_aggrevate]\n",
    "active_policies = [active_dagger, active_aggrevate, dagger, aggrevate, unc_active_dagger, unc_active_aggrevate]\n",
    "policies = innactive_policies + active_policies\n",
    "\n",
    "\n",
    "# aggregate initial data\n",
    "for policy in policies:\n",
    "    policy.aggregate((obs_data,act_data))\n",
    " \n",
    "for i in tqdm(range(num_episodes)): #Dagger main loop\n",
    "    for policy in policies:\n",
    "        policy.init_policy()\n",
    "        if policy.trainable:\n",
    "            if policy.constrained:\n",
    "                policy.fit_policy(constraints=(None,None))\n",
    "            else:\n",
    "                policy.fit_policy()\n",
    "            \n",
    "    new_observations, new_actions, Vs = ([] for _ in range(3))\n",
    "    \n",
    "    for j in tqdm(range(num_rollouts), position=1, desc='rollout', leave=False):\n",
    "        for policy in policies:\n",
    "            policy.reset_env()\n",
    "            \n",
    "        r_t = []\n",
    "        d_i = 0\n",
    "        for k in range(max_steps):            \n",
    "            for policy in policies:\n",
    "                expert_action_logits, expert_action, _ = expert.take_action(policy.cur_obs)\n",
    "                policy.expert_observations.append(policy.cur_obs)\n",
    "                policy.expert_actions.append(expert_action_logits)\n",
    "                if policy.active:\n",
    "                    self_action_logits, self_action, conf = policy.take_action(policy.cur_obs) \n",
    "                    policy.self_observations.append(policy.cur_obs)\n",
    "                    policy.self_actions.append(self_action_logits)\n",
    "                    query = random.bernoulli(rng, p=b/(b + conf.item()))\n",
    "                    policy.cur_active_mask.append(query or (i==0))\n",
    "                    query_self = random.bernoulli(rng, p=b2/(b2 + conf.item()))\n",
    "                    policy.cur_self_mask.append(~query and ~query_self)\n",
    "                \"\"\"\n",
    "                if i < 2 or (policy.cur_obs[3] > 0):\n",
    "                    policy.expert_observations.append(policy.cur_obs)\n",
    "                    policy.expert_actions.append(expert_action_logits)\n",
    "                    if policy.active:\n",
    "                        self_action_logits, self_action, conf = policy.take_action(policy.cur_obs) \n",
    "                        policy.self_observations.append(policy.cur_obs)\n",
    "                        policy.self_actions.append(self_action_logits)\n",
    "                        query = 1\n",
    "                        policy.cur_active_mask.append(query or (i==0))\n",
    "                        query_self = random.bernoulli(rng, p=b2/(b2 + conf.item()))\n",
    "                        policy.cur_self_mask.append((not query) and (not query_self))\n",
    "                else:\n",
    "                    policy.expert_observations.append(policy.cur_obs)\n",
    "                    policy.expert_actions.append(expert_action_logits)\n",
    "                    if policy.active:\n",
    "                        self_action_logits, self_action, conf = policy.take_action(policy.cur_obs) \n",
    "                        policy.self_observations.append(policy.cur_obs)\n",
    "                        policy.self_actions.append(self_action_logits)\n",
    "                        query = 0\n",
    "                        query_self = 1\n",
    "                        if policy.name not in [\"dagger\",\"aggrevate\"]:\n",
    "                            policy.cur_active_mask.append(query or (i==0))\n",
    "                            query_self = random.bernoulli(rng, p=b2/(b2 + conf.item()))\n",
    "                        policy.cur_self_mask.append((not query) and (not query_self))                         \n",
    "                \"\"\"\n",
    "\n",
    "            doneall=True\n",
    "            for policy in policies:\n",
    "                if not policy.done:\n",
    "                    obs, r, done = policy.step(k+1)\n",
    "                    policy.r_t.append(r)\n",
    "                    doneall=False\n",
    "                \n",
    "            if doneall: break\n",
    "\n",
    "        for policy in policies:\n",
    "            policy.rewards[-1].append(policy.reward_accum)\n",
    "\n",
    "    for policy in tqdm(policies,desc='data aggregation', position=2, leave=False):\n",
    "        policy.aggregate()\n",
    "    #Parallel(n_jobs=-1, prefer=\"threads\")(delayed(policy.aggregate()) for policy in policies)\n",
    "    \n",
    "    print('Epoch: {}'.format(i))\n",
    "    for policy in policies:\n",
    "        print('{}: {} {}'.format(policy.name, np.mean(policy.rewards[-1]), policy.num_data[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_evaluate():\n",
    "    #make_plot([dagger,expert,active_dagger, aggrevate, active_aggrevate],[('noisy expert',noisy_expert_rewards, expert.num_data)])\n",
    "    make_plot([dagger,expert, active_dagger, aggrevate, active_aggrevate, unc_active_dagger, unc_active_aggrevate],[])\n",
    "    \n",
    "    for policy in [dagger,expert,active_dagger, aggrevate, active_aggrevate]:\n",
    "        print(policy.name,[np.mean(x) for x in policy.rewards], policy.num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dagger.active_mask.sum(), active_dagger.self_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole\n",
    "plot_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_aggrevate.save('./models/','test_policy_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_model = './models/{}_active_dagger_model.h5'.format(j)\n",
    "final_model = './models/test_policy_class.h5'\n",
    "!xvfb-run -s \"-screen 0 600x400x24\" python3.6 render.py --mpath $final_model --envname HalfCheetah-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"./gym-results/\"):\n",
    "    if file.endswith(\".mp4\"):   \n",
    "        mp4name = os.path.join(\"./gym-results\", file)\n",
    "        print(mp4name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = io.open(mp4name, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
